import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.feature_selection import VarianceThreshold
from scipy.stats import pearsonr
from sklearn.model_selection import KFold
from sklearn.linear_model import Ridge
import pickle
import os
import gc

# --- Helper Functions ---
def train_fallback_gbdt(X, y, X_test, n_folds=5):
    lgb_params = {
        'objective': 'regression', 'metric': 'mse', 'num_leaves': 64, 'learning_rate': 0.02,
        'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 5, 'min_data_in_leaf': 50,
        'lambda_l1': 0.5, 'lambda_l2': 0.5, 'verbose': -1
    }
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    models = []
    scores = []
    test_preds = np.zeros(len(X_test))
    
    for train_idx, val_idx in kf.split(X):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        train_data = lgb.Dataset(X_train, label=y_train)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
        model = lgb.train(lgb_params, train_data, num_boost_round=2000, valid_sets=[val_data], 
                         callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)])
        y_pred = model.predict(X_val)
        y_pred -= y_pred.mean()
        score = weighted_pearson_correlation(y_val, y_pred)
        scores.append(score)
        models.append(model)
        test_preds += model.predict(X_test) / n_folds
        gc.collect()
    
    return models, test_preds

def weighted_pearson_correlation(y_true, y_pred, weights=None):
    if weights is None:
        weights = np.ones(len(y_true))
    y_true = np.nan_to_num(y_true, nan=0, posinf=0, neginf=0)
    y_pred = np.nan_to_num(y_pred, nan=0, posinf=0, neginf=0)
    weighted_mean_true = np.average(y_true, weights=weights)
    weighted_mean_pred = np.average(y_pred, weights=weights)
    numerator = np.sum(weights * (y_true - weighted_mean_true) * (y_pred - weighted_mean_pred))
    denominator = np.sqrt(np.sum(weights * (y_true - weighted_mean_true)**2) * np.sum(weights * (y_pred - weighted_mean_pred)**2))
    return numerator / denominator if denominator != 0 else 0

# --- Step 1: Load Processed Data and Models ---
train_path = '/kaggle/working/train_processed.parquet'
test_path = '/kaggle/working/test_processed.parquet'
pca_path = '/kaggle/working/pca.pkl'
scaler_path = '/kaggle/working/scaler.pkl'
submission_path = '/kaggle/working/submission.csv'

for path in [train_path, test_path, pca_path, scaler_path]:
    if not os.path.exists(path):
        raise FileNotFoundError(f"File not found at {path}.")

train_data = pd.read_parquet(train_path).astype('float32')
test_data = pd.read_parquet(test_path).astype('float32')
with open(pca_path, 'rb') as f:
    pca_model = pickle.load(f)
with open(scaler_path, 'rb') as f:
    scaler = pickle.load(f)

# Define feature set based on your preprocessing
features = [
    'log_volume', 'volume_sq', 'volume_cube', 'volume_sin', 'volume_exp', 'volume_roll_mean', 'volume_roll_std',
    'log_buy_qty', 'buy_qty_sq', 'buy_qty_cube', 'buy_qty_sin', 'buy_qty_exp', 'buy_qty_roll_mean', 'buy_qty_roll_std',
    'log_sell_qty', 'sell_qty_sq', 'sell_qty_cube', 'sell_qty_sin', 'sell_qty_exp', 'sell_qty_roll_mean', 'sell_qty_roll_std',
    'buy_sell_ratio', 'net_flow', 'order_imbalance', 'volume_imbalance', 'X680_lag_1', 'X680_lag_2', 'X680_lag_3'
] + [f'pca_{i}' for i in range(20)]

# --- Step 2: Feature Selection with Variance Threshold and Adversarial Validation ---
def select_top_features(X, y, top_n=50):
    selector = VarianceThreshold(threshold=1e-5)
    X_selected = selector.fit_transform(X)
    selected_features = X.columns[selector.get_support()].tolist()
    
    try:
        train_data_lgb = lgb.Dataset(X_selected, label=y)
        params = {
            'objective': 'regression', 'metric': 'mse', 'num_leaves': 31, 'learning_rate': 0.05,
            'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'verbose': -1
        }
        model = lgb.train(params, train_data_lgb, num_boost_round=100)
        importance = pd.Series(model.feature_importance(importance_type='gain'), index=selected_features)
        top_features = importance.nlargest(min(top_n, len(selected_features))).index.tolist()
        
        correlations = {}
        for col in selected_features:
            try:
                corr, _ = pearsonr(X[col], y)
                correlations[col] = abs(corr)
            except:
                correlations[col] = 0
        corr_features = sorted(correlations, key=correlations.get, reverse=True)[:min(top_n, len(selected_features))]
        
        combined_features = list(set(top_features + corr_features))[:top_n]
        return combined_features
    except:
        return sorted(correlations, key=correlations.get, reverse=True)[:top_n]

def adversarial_validation(train_data, test_data, features, threshold=0.7):
    X = train_data[features].copy().astype('float32').fillna(0)
    y = train_data['label'].fillna(0).astype('float32')
    
    selector = VarianceThreshold(threshold=1e-5)
    X_selected = selector.fit_transform(X)
    stable_features = [features[i] for i in selector.get_support(indices=True)]
    
    correlations = {}
    for col in stable_features:
        try:
            corr, _ = pearsonr(X[col], y)
            correlations[col] = abs(corr)
        except:
            correlations[col] = 0
    
    stable_features = [f for f in stable_features if correlations[f] > 0.01 or 'pca' in f]
    return stable_features

available_features = [f for f in features if f in train_data.columns]
stable_features = adversarial_validation(train_data, None, available_features, threshold=0.7)
features = [f for f in available_features if f in stable_features] + [f'pca_{i}' for i in range(20) if f'pca_{i}' in train_data.columns]

X = train_data[features].fillna(0).astype('float32')
y = train_data['label'].fillna(0).astype('float32')
selected_features = select_top_features(X, y, top_n=50)
del X
gc.collect()

X_selected = train_data[selected_features].fillna(0).astype('float32')
X_test = test_data[selected_features].fillna(0).astype('float32')

known_features = [
    'log_volume', 'volume_sq', 'volume_cube', 'volume_sin', 'volume_exp', 'volume_roll_mean', 'volume_roll_std',
    'log_buy_qty', 'buy_qty_sq', 'buy_qty_cube', 'buy_qty_sin', 'buy_qty_exp', 'buy_qty_roll_mean', 'buy_qty_roll_std',
    'log_sell_qty', 'sell_qty_sq', 'sell_qty_cube', 'sell_qty_sin', 'sell_qty_exp', 'sell_qty_roll_mean', 'sell_qty_roll_std',
    'buy_sell_ratio', 'net_flow', 'order_imbalance', 'volume_imbalance'
]
known_features = [f for f in known_features if f in train_data.columns]
fallback_models, fallback_preds = train_fallback_gbdt(train_data[known_features], y, test_data[known_features])

# --- Step 3: Train and Evaluate Models ---
lgb_params = {
    'objective': 'regression', 'metric': 'mse', 'num_leaves': 128, 'learning_rate': 0.02,
    'max_depth': 12, 'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'bagging_freq': 5,
    'min_data_in_leaf': 50, 'lambda_l1': 0.5, 'lambda_l2': 0.5, 'verbose': -1
}

n_folds = 5
kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
lgb_scores, ridge_scores, ensemble_scores = [], [], []
lgb_models, ridge_models = [], []

for train_idx, val_idx in kf.split(X_selected):
    X_train, X_val = X_selected.iloc[train_idx], X_selected.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
    
    train_data_lgb = lgb.Dataset(X_train, label=y_train)
    val_data_lgb = lgb.Dataset(X_val, label=y_val, reference=train_data_lgb)
    lgb_model = lgb.train(lgb_params, train_data_lgb, num_boost_round=3000, valid_sets=[val_data_lgb], 
                         callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)])
    y_pred_lgb = lgb_model.predict(X_val)
    y_pred_lgb -= y_pred_lgb.mean()
    lgb_score = weighted_pearson_correlation(y_val, y_pred_lgb)
    lgb_scores.append(lgb_score)
    lgb_models.append(lgb_model)
    gc.collect()
    
    ridge = Ridge(alpha=10.0, random_state=42)
    ridge.fit(X_train, y_train)
    y_pred_ridge = ridge.predict(X_val)
    y_pred_ridge -= y_pred_ridge.mean()
    ridge_score = weighted_pearson_correlation(y_val, y_pred_ridge)
    ridge_scores.append(ridge_score)
    ridge_models.append(ridge)
    gc.collect()
    
    if lgb_scores[-1] > 0 and ridge_scores[-1] > 0:
        y_pred_ensemble = (0.7 * y_pred_lgb + 0.3 * y_pred_ridge)
        y_pred_ensemble -= y_pred_ensemble.mean()
        ensemble_score = weighted_pearson_correlation(y_val, y_pred_ensemble)
        ensemble_scores.append(ensemble_score)
    else:
        ensemble_scores.append(0)
    
    del X_train, X_val, y_train, y_val
    gc.collect()

mean_lgb_score = np.mean(lgb_scores)
mean_ridge_score = np.mean(ridge_scores)
mean_ensemble_score = np.mean([s for s in ensemble_scores if s > 0]) if any(s > 0 for s in ensemble_scores) else 0

# --- Step 4: Meta-Ridge Ensembling ---
meta_X = np.zeros((len(X_selected), len(lgb_models) + len(ridge_models)))
meta_X_test = np.zeros((len(X_test), len(lgb_models) + len(ridge_models)))

for i, model in enumerate(lgb_models):
    meta_X[:, i] = model.predict(X_selected)
    meta_X_test[:, i] = model.predict(X_test)
for i, model in enumerate(ridge_models, len(lgb_models)):
    meta_X[:, i] = model.predict(X_selected)
    meta_X_test[:, i] = model.predict(X_test)

meta_ridge = Ridge(alpha=1.0, random_state=42)
meta_ridge.fit(meta_X, y)
ensemble_preds = meta_ridge.predict(meta_X_test)
ensemble_preds -= ensemble_preds.mean()

# Blend with fallback GBDT
final_preds = 0.7 * ensemble_preds + 0.3 * fallback_preds
final_preds -= final_preds.mean()

# --- Step 5: Generate Submission ---
row_id = test_data.index  # Using index as row_id since your preprocessing script uses index
submission = pd.DataFrame({'row_id': row_id, 'label': final_preds})
submission.to_csv(submission_path, index=False)

# Clear memory
del train_data, test_data, X_selected, X_test, y, lgb_models, ridge_models, meta_X, meta_X_test
gc.collect()
